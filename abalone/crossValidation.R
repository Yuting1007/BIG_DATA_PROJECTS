library(tidyverse)
library(ggplot2)
library(caret)
library(e1071)
library(ranger)

data <- read.table("https://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data", 
                   sep = ",")
names<-c("sex","lenght","diam","ht","wwt","scwt","vcwt","shwt","rings")

colnames(data)<-names
# Attribute information:
#   
#   Given is the attribute name, attribute type, the measurement unit and a
# brief description.  The number of rings is the value to predict: either
# as a continuous value or as a classification problem.
# 
# Name		Data Type	Meas.	Description
# ----		---------	-----	-----------
# Sex		nominal			M, F, and I (infant)
# Length		continuous	mm	Longest shell measurement
# Diameter	continuous	mm	perpendicular to length
# Height		continuous	mm	with meat in shell
# Whole weight	continuous	grams	whole abalone
# Shucked weight	continuous	grams	weight of meat
# Viscera weight	continuous	grams	gut weight (after bleeding)
# Shell weight	continuous	grams	after being dried
# Rings		integer			+1.5 gives the age in years

####

# > str(data)
# 'data.frame':	4177 obs. of  9 variables:
#   $ sex   : Factor w/ 3 levels "F","I","M": 3 3 1 3 2 2 1 1 3 1 ...
# $ lenght: num  0.455 0.35 0.53 0.44 0.33 0.425 0.53 0.545 0.475 0.55 ...
# $ diam  : num  0.365 0.265 0.42 0.365 0.255 0.3 0.415 0.425 0.37 0.44 ...
# $ ht    : num  0.095 0.09 0.135 0.125 0.08 0.095 0.15 0.125 0.125 0.15 ...
# $ wwt   : num  0.514 0.226 0.677 0.516 0.205 ...
# $ scwt  : num  0.2245 0.0995 0.2565 0.2155 0.0895 ...
# $ vcwt  : num  0.101 0.0485 0.1415 0.114 0.0395 ...
# $ shwt  : num  0.15 0.07 0.21 0.155 0.055 0.12 0.33 0.26 0.165 0.32 ...
# $ rings : int  15 7 9 10 7 8 20 16 9 19 ...

# There are a number of pre-processing steps that are easily implemented by caret. 
# Several stand-alone functions from caret target specific issues that might arise when 
# setting up the model. These include
# dummyVars: creating dummy variables from categorical variables with multiple categories
# nearZeroVar: identifying zero- and near zero-variance predictors 
# (these may cause issues when subsampling)
# findCorrelation: identifying correlated predictors
# findLinearCombos: identify linear dependencies between predictors
#######################
## preProcess function
#######################
# In addition to these individual functions, there also exists the preProcess function 
# which can be used to perform more common tasks such as centering and scaling, 
# imputation and transformation. 
# preProcess takes in a data frame to be processed and a method which can be any of 
# â€œBoxCoxâ€?, â€œYeoJohnsonâ€?, â€œexpoTransâ€?, â€œcenterâ€?, â€œscaleâ€?, â€œrangeâ€?, â€œknnImputeâ€?, â€œbagImputeâ€?, 
# â€œmedianImputeâ€?, â€œpcaâ€?, â€œicaâ€?, â€œspatialSignâ€?, â€œcorrâ€?, â€œzvâ€?, â€œnzvâ€?, and â€œconditionalXâ€?.


# add a logical variable for "old" (age > 10) ## rings report age!!
data <- data %>%
  mutate(old = rings > 10) #%>%
#rings indicate age
# remove the "rings" variable
# select(rings)

# imputation and transformation. 
# preProcess takes in a data frame to be processed and a method which can be any of 
# â€œBoxCoxâ€?, â€œYeoJohnsonâ€?, â€œexpoTransâ€?, â€œcenterâ€?, â€œscaleâ€?, â€œrangeâ€?, â€œknnImputeâ€?, â€œbagImputeâ€?, 
# â€œmedianImputeâ€?, â€œpcaâ€?, â€œicaâ€?, â€œspatialSignâ€?, â€œcorrâ€?, â€œzvâ€?, â€œnzvâ€?, and â€œconditionalXâ€?.

#near zero variance
#normalize the data and clean up the outilers
data.par<-preProcess(data,
                   method = c("center", "scale", "YeoJohnson", "nzv"))
## created parameters to be applied to the data

data.Transformed <- predict(data.par, data)
## easy remove outliers from transformed data
## everything is centered and scaled - assuming normal distribution - 
## we can drop all data points outside of the 3 standard deviations = +/- 3 for standard normal

rm.out<-function(x)(ifelse(x >3, NA,x ))
#if x>3 replace it byy NA, else x
data.tr<-rm.out(as.matrix(data.Transformed[,2:9]))
# center, scale and perform a YeoJohnson transformation
# identify and remove variables with near zero variance

# transform back to df and add the non-numeric vars
#back to dataframe
data.1<-as.data.frame(data.tr)
data.2<-cbind(data[,-c(2:9)],data.1)


# For more info on preProcess see
# https://topepo.github.io/caret/pre-processing.html

###################
#### DATA SPLITTING
###################
#into traing and testing sets
# split into training and testing using an index generated by randomly sampling 90% of the data
# 0.9 * 4177 = 3759 , it will round down
# remove rings from the data since we will work with old

set.seed(42)
train_index <- sample(1:nrow(data.2[,-10]), 0.9 * nrow(data.2[,-10]))
#train <- data.2[train_index,-10 ]
#test <- data.2[-train_index,-10 ]

# view the first 6 rows of the training data
str(train)

## we can also use the createDataPartition function from the package, see
## http://www.rebeccabarter.com/blog/2017-11-17-caret_tutorial/

train_index <- createDataPartition(data.2$old,
                                   p = 0.8, # train : test ratio - 80%- 20%
                                   list = FALSE,
                                   times = 10) # do it 10 times to create 10 folds
#dropping the rings
#he response variable and are high related with old
#contain one in a time
train <- data.2[train_index, -10 ]
test <- data.2[-train_index, -10 ]

###### MODELING ##### 
## Now lets fit some  models to predict "old" from the rest of the variables
## USES DATA SPLIT
methodList<-c( "glm",# generalized linear model
               "gam"# generalized additive model
)

set.seed(42)
#sample function to smaple the index
train_index <- sample(1:nrow(data.2[,-10]), 0.9 * nrow(data.2[,-10]))
train <- data.2[train_index, -10]
test <- data.2[-train_index, -10]
#missleading name
#just like train_index
TestRows<-train_index
ds<-data.2[,-10]

for ( m in (1:length(methodList))){
  TrainData    <- ds[-TestRows,c(3:9)] #-TestRows?
  TrainClasses <- ds[-TestRows,2]
  TestData     <- ds[TestRows,c(3:9)]
  TestClasses  <- ds[TestRows,2]

  ## hyperparameter optimization
  Fit <- train(x=TrainData, y=as.factor(TrainClasses),
               #trControl=train_control,
               method = methodList[m],
               tuneLength = 11,
               trace = TRUE,
               maxit = 100)

  confusionMatrix(Fit)
  
  #To Plot Observed vs Predicted in the Test data
  pred.1<-as.data.frame(predict(Fit,TestData))
  #-> means assign
  rownames(pred.1)->pred.1$CID#predict
  rownames(TestData)->testSampleList#observed
  #% means in-search
  out.1<-ds[which(rownames(ds) %in% testSampleList),]
  rownames(out.1)->out.1$CID#observed
  merge(out.1,pred.1,by="CID")->xx#observed and predicted
  
  # plot(xx[,3]~xx[,11]) observed vs predicted
  p1<-ggplot(xx, aes(x=xx[,3], y=xx[,11])) + 
    geom_point()
  xx$set<-"test"
  colnames(xx)[15]<-"prediction"
}


##################
#cross validation

#Repeated k-fold Cross Validation
train_control <- trainControl(method="repeatedcv", number=10, repeats=3)

#Leave One Out Cross Validation
train_control <- trainControl(method="LOOCV")

#Bootstrap
train_control <- trainControl(method="boot", number=100)


## USE THESE Validation methods in the loop below
# train the model syntax is 
# model <- train(Species~., data=iris, trControl=train_control, method="nb")
# would require missing values to be removed or imputed

ds<- na.omit(data.2[,-2])
# drops to 4162 from 4177

for ( m in (1:length(methodList1))){
  ## hyperparameter optimization
  Fit <- train(rings~.,data=ds, 
               trControl=train_control,
               method = methodList[m])

  #To Plot Observed vs Predicted in the Test data
  pred.1<-as.data.frame(predict(Fit,TestData))
  rownames(pred.1)->pred.1$CID
  rownames(TestData)->testSampleList
  out.1<-ds[which(rownames(ds) %in% testSampleList),]
  rownames(out.1)->out.1$CID
  merge(out.1,pred.1,by="CID")->xx
  
  # plot(xx[,3]~xx[,11]) observed vs predicted
  p1<-ggplot(xx, aes(x=xx[,3], y=xx[,11])) + 
    geom_point()
}



## What if we wanted to old age?
## we would want to remove the rings from the predictor list


TestRows<-train_index
ds<-data.2[,-10]

for ( m in (1:length(methodList1))){
  TrainData    <- ds[-TestRows,-c(2)]
  TrainClasses <- ds[-TestRows,2]
  TestData     <- ds[TestRows,-c(2)]
  TestClasses  <- ds[TestRows,2]
  
  ## hyperparameter optimization
  Fit <- train(x=TrainData, y=as.factor(TrainClasses),
               method = methodList[m],
               tuneLength = 11,
               trace = TRUE,
               maxit = 100)
  
  confusionMatrix(Fit) #only valid for classification output
  
  #To Plot Observed vs Predicted in the Test data
  pred.1<-as.data.frame(predict(Fit,TestData))
  rownames(pred.1)->pred.1$CID
  rownames(TestData)->testSampleList
  out.1<-ds[which(rownames(ds) %in% testSampleList),]
  rownames(out.1)->out.1$CID
  merge(out.1,pred.1,by="CID")->xx
  
  # plot(xx[,3]~xx[,11]) observed vs predicted
  p1<-ggplot(xx, aes(x=xx[,10], y=xx[,11])) + 
    geom_point()
}
